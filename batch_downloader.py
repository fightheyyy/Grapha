from selenium import webdriver
from selenium.webdriver.edge.service import Service as EdgeService
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import time
import os

# è®¾ç½® Edge æµè§ˆå™¨é€‰é¡¹
edge_options = webdriver.EdgeOptions()
edge_options.add_argument('--disable-gpu')
edge_options.add_argument('--no-sandbox')
edge_options.add_argument('--headless')  # å¯é€‰ï¼šæ— å¤´æ¨¡å¼åå°è¿è¡Œ

# è‡ªåŠ¨ä¸‹è½½é€‚é…ç‰ˆæœ¬çš„ EdgeDriverï¼ˆæ¨èï¼‰
try:
    from webdriver_manager.microsoft import EdgeChromiumDriverManager
    service = EdgeService(EdgeChromiumDriverManager().install())
except Exception as e:
    print("âš ï¸ webdriver-manager åŠ è½½å¤±è´¥ï¼Œå°è¯•æ‰‹åŠ¨æŒ‡å®šè·¯å¾„...")
    service = EdgeService(executable_path='msedgedriver')

# åˆ›å»º Edge æµè§ˆå™¨é©±åŠ¨
driver = webdriver.Edge(service=service, options=edge_options)

# åŸºç¡€ç½‘å€
base_url = "https://www.mohurd.gov.cn/gongkai/zhengce/gzk/index.html"

# åˆ›å»ºè¾“å‡ºæ–‡ä»¶ç›®å½•
output_dir = "mohurd_policy_details2"
os.makedirs(output_dir, exist_ok=True)
output_file = os.path.join(output_dir, 'policy_detail_urls.txt')


def get_titles_and_detail_links():
    """ä»å½“å‰é¡µé¢ä¸­æå–æ”¿ç­–æ ‡é¢˜å’Œå¯¹åº”çš„è¯¦æƒ…é¡µé“¾æ¥"""
    html = driver.page_source
    soup = BeautifulSoup(html, 'html.parser')

    titles = []
    links = []

    for a in soup.find_all('a', href=True):
        href = urljoin(base_url, a['href'])

        # åªä¿ç•™ç¬¦åˆè¯¦æƒ…é¡µæ ¼å¼çš„ URL
        if "/gongkai/zhengce/gzk/art/" in href:
            title = a.get_text(strip=True)
            titles.append(title)
            links.append(href)

    return titles, links


def save_to_file(titles, links):
    """å°†æ ‡é¢˜ä¸é“¾æ¥å†™å…¥æ–‡ä»¶"""
    with open(output_file, 'a', encoding='utf-8') as f:
        for title, link in zip(titles, links):
            line = f"{title} - {link}\n"
            f.write(line)
            print(f"ğŸ“Œ å·²ä¿å­˜ï¼š{line.strip()}")
    print(f"âœ… å½“å‰é¡µå…±ä¿å­˜äº† {len(titles)} æ¡è®°å½•ã€‚")


def click_next_page_button():
    """å°è¯•ç‚¹å‡»â€œä¸‹ä¸€é¡µâ€æŒ‰é’®"""
    try:
        next_button = WebDriverWait(driver, 10).until(
            EC.element_to_be_clickable((By.XPATH, '//a[contains(text(), "ä¸‹ä¸€é¡µ")]'))
        )
        driver.execute_script("arguments[0].scrollIntoView();", next_button)
        next_button.click()
        time.sleep(3)  # ç­‰å¾…æ–°å†…å®¹åŠ è½½
        return True
    except Exception as e:
        print("â›” æ— æ³•æ‰¾åˆ°â€˜ä¸‹ä¸€é¡µâ€™æŒ‰é’®ï¼Œå¯èƒ½å·²æ˜¯æœ€åä¸€é¡µã€‚")
        return False


def crawl_pages(max_pages=10):
    current_page = 1
    total_count = 0

    while current_page <= max_pages:
        print(f"\nğŸ“˜ æ­£åœ¨å¤„ç†ç¬¬ {current_page} é¡µ...")

        titles, detail_links = get_titles_and_detail_links()

        if not titles:
            print("âŒ å½“å‰é¡µæ²¡æœ‰æ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„æ”¿ç­–ä¿¡æ¯ã€‚")
            break

        save_to_file(titles, detail_links)
        total_count += len(titles)

        # å°è¯•ç¿»é¡µ
        success = click_next_page_button()
        if not success:
            print("ğŸ”š å·²åˆ°æœ€åä¸€é¡µï¼Œåœæ­¢ç¿»é¡µã€‚")
            break

        current_page += 1

    print(f"\nğŸ‰ å…±æå–å¹¶ä¿å­˜ {total_count} æ¡æ”¿ç­–ä¿¡æ¯ã€‚")


def main():
    print("ğŸ“˜ å¼€å§‹æ‰¹é‡æå–ä½å»ºéƒ¨è§„ç« åº“çš„æ”¿ç­–æ ‡é¢˜åŠè¯¦æƒ…é¡µé“¾æ¥...")
    driver.get(base_url)
    time.sleep(5)  # åˆå§‹ç­‰å¾…åŠ è½½

    crawl_pages(max_pages=10)

    print(f"ğŸ æ‰€æœ‰æ•°æ®å·²ä¿å­˜è‡³ï¼š{output_file}")
    driver.quit()


if __name__ == "__main__":
    main()