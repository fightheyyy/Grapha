from selenium import webdriver
from selenium.webdriver.edge.service import Service as EdgeService
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
from urllib.parse import urljoin, unquote, urlparse, parse_qs
import os
import time
import requests

# è®¾ç½® Edge æµè§ˆå™¨é€‰é¡¹
edge_options = webdriver.EdgeOptions()
edge_options.add_argument('--disable-gpu')
edge_options.add_argument('--no-sandbox')
edge_options.add_argument('--headless')  # å¯é€‰ï¼šæ— å¤´æ¨¡å¼åå°è¿è¡Œ

# è‡ªåŠ¨ä¸‹è½½é€‚é…ç‰ˆæœ¬çš„ EdgeDriverï¼ˆæ¨èï¼‰
try:
    from webdriver_manager.microsoft import EdgeChromiumDriverManager
    service = EdgeService(EdgeChromiumDriverManager().install())
except Exception as e:
    print("âš ï¸ webdriver-manager åŠ è½½å¤±è´¥ï¼Œå°è¯•æ‰‹åŠ¨æŒ‡å®šè·¯å¾„...")
    service = EdgeService(executable_path='msedgedriver')

# åˆ›å»º Edge æµè§ˆå™¨é©±åŠ¨
driver = webdriver.Edge(service=service, options=edge_options)

# åŸºç¡€ç½‘å€
base_url = "https://www.mohurd.gov.cn/gongkai/zhengce/gzk/index.html"

# åˆ›å»ºä¸‹è½½ç›®å½•
output_dir = "mohurd_word_documents2"
os.makedirs(output_dir, exist_ok=True)

headers = {
    "User-Agent": "Mozilla/5.0"
}

def get_text_version_links():
    """æå–å½“å‰é¡µé¢ä¸­çš„â€˜æ–‡å­—ç‰ˆâ€™é“¾æ¥"""
    html = driver.page_source
    soup = BeautifulSoup(html, 'html.parser')

    text_version_links = []
    for a in soup.find_all('a', href=True):
        if 'æ–‡å­—ç‰ˆ' in a.get_text(strip=True):
            full_url = urljoin(base_url, a['href'])
            text_version_links.append(full_url)
    return text_version_links


def get_detail_page_links():
    """ç²¾ç¡®æå–å½“å‰é¡µé¢ä¸­æ¯ä¸ªæ”¿ç­–çš„è¯¦æƒ…é¡µé“¾æ¥"""
    html = driver.page_source
    soup = BeautifulSoup(html, 'html.parser')

    detail_links = []
    for a in soup.find_all('a', href=True):
        href = urljoin(base_url, a['href'])

        # æ’é™¤ javascript é“¾æ¥
        if href.startswith("javascript:"):
            continue

        # æ’é™¤ä¸‹è½½é“¾æ¥å’Œå›¾ç‰‡ç‰ˆé“¾æ¥
        if "/downloadWord" in href or "fileUrl=" in href or "viewType=1" in href:
            continue

        # åªä¿ç•™ç¬¦åˆè¯¦æƒ…é¡µæ ¼å¼çš„é“¾æ¥
        if "/gongkai/zhengce/gzk/art/" in href:
            detail_links.append(href)

    return detail_links

def get_filename_from_url(url):
    """ä» URL ä¸­æå–å¹¶è§£ç æ–‡ä»¶å"""
    parsed_url = urlparse(url)
    query_params = parse_qs(parsed_url.query)
    file_name = query_params.get('fileName', ['æœªå‘½åæ–‡æ¡£'])[0]
    return unquote(file_name)


def download_word_document(doc_url):
    """ä¸‹è½½ Word æ–‡æ¡£"""
    try:
        response = requests.get(doc_url, stream=True, headers=headers, timeout=30)
        response.raise_for_status()

        # æå–æ–‡ä»¶åå¹¶æ·»åŠ  .doc åç¼€
        filename = get_filename_from_url(doc_url)
        safe_filename = "".join(c for c in filename if c.isalnum() or c in (' ', '-', '_', '.'))
        file_path = os.path.join(output_dir, f"{safe_filename}.docx")

        print(f"æ­£åœ¨ä¸‹è½½ï¼š{safe_filename}.docx æ¥æºï¼š{doc_url}")
        with open(file_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=1024 * 1024):  # æ¯æ¬¡è¯»å– 1MB
                f.write(chunk)
        print(f"å·²ä¿å­˜ï¼š{file_path}")

    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥ï¼š{doc_url}, é”™è¯¯ï¼š{e}")


def get_title_from_detail_page(url):
    """è®¿é—®è¯¦æƒ…é¡µå¹¶æå–ç½‘é¡µæ ‡é¢˜ä½œä¸ºæ–‡æ¡£å"""
    try:
        driver.execute_script("window.open('');")
        driver.switch_to.window(driver.window_handles[-1])
        driver.get(url)
        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))
        title = driver.title.strip()
        driver.close()
        driver.switch_to.window(driver.window_handles[0])
        return title
    except Exception as e:
        print(f"âŒ è·å–è¯¦æƒ…é¡µæ ‡é¢˜å¤±è´¥ï¼š{url}, é”™è¯¯ï¼š{e}")
        return "æœªå‘½åæ–‡æ¡£"


def save_detail_urls_to_file(urls, filename='policy_detail_urls.txt'):
    """å°†è¯¦æƒ…é¡µé“¾æ¥å’Œå¯¹åº”æ–‡æ¡£åå†™å…¥æ–‡ä»¶"""
    with open(filename, 'a', encoding='utf-8') as f:
        for url in urls:
            title = get_title_from_detail_page(url)
            line = f"{title} - {url}\n"
            f.write(line)
            print(f"ğŸ“Œ å·²ä¿å­˜ï¼š{line.strip()}")
    print(f"âœ… å·²å°† {len(urls)} æ¡è®°å½•å†™å…¥ {filename}")


def click_next_page_button():
    """å°è¯•ç‚¹å‡»â€œä¸‹ä¸€é¡µâ€æŒ‰é’®"""
    try:
        next_button = WebDriverWait(driver, 10).until(
            EC.element_to_be_clickable((By.XPATH, '//a[contains(text(), "ä¸‹ä¸€é¡µ")]'))
        )
        driver.execute_script("arguments[0].scrollIntoView();", next_button)
        next_button.click()
        time.sleep(3)  # ç­‰å¾…æ–°å†…å®¹åŠ è½½
        return True
    except Exception as e:
        print("â›” æ— æ³•æ‰¾åˆ°â€˜ä¸‹ä¸€é¡µâ€™æŒ‰é’®ï¼Œå¯èƒ½å·²æ˜¯æœ€åä¸€é¡µã€‚")
        return False


def crawl_pages_with_selenium(max_pages=10):
    current_page = 1
    downloaded_count = 0

    while current_page <= max_pages:
        print(f"\nğŸ“˜ æ­£åœ¨å¤„ç†ç¬¬ {current_page} é¡µ...")

        # è·å–â€œæ–‡å­—ç‰ˆâ€é“¾æ¥ï¼ˆç”¨äºä¸‹è½½ï¼‰
        doc_links = get_text_version_links()

        # è·å–â€œæ ‡é¢˜â€é“¾æ¥ï¼ˆç”¨äºè¯¦æƒ…é¡µï¼‰
        detail_links = get_detail_page_links()

        if not doc_links:
            print("âŒ å½“å‰é¡µæ²¡æœ‰æ‰¾åˆ°ä»»ä½•â€˜æ–‡å­—ç‰ˆâ€™é“¾æ¥ã€‚")
            break

        print(f"âœ… ç¬¬ {current_page} é¡µå…±æ‰¾åˆ° {len(doc_links)} ä¸ªâ€˜æ–‡å­—ç‰ˆâ€™é“¾æ¥ï¼Œ{len(detail_links)} ä¸ªè¯¦æƒ…é¡µé“¾æ¥ã€‚")

        # æ‰“å°è¯¦æƒ…é¡µé“¾æ¥ï¼Œå¹¶ä¸‹è½½ Word æ–‡æ¡£
        for idx, (doc_url, detail_url) in enumerate(zip(doc_links, detail_links)):
            print(f"ğŸ“Œ ç¬¬ {idx + 1} æ¡æ”¿ç­–è¯¦æƒ…é¡µé“¾æ¥ï¼š{detail_url}")
            download_word_document(doc_url)
            downloaded_count += 1

        # ä¿å­˜è¯¦æƒ…é¡µé“¾æ¥åˆ°æ–‡ä»¶
        save_detail_urls_to_file(detail_links)

        # å°è¯•ç¿»é¡µ
        success = click_next_page_button()
        if not success:
            print("ğŸ”š å·²åˆ°æœ€åä¸€é¡µï¼Œåœæ­¢ç¿»é¡µã€‚")
            break

        current_page += 1

    print(f"\nğŸ‰ å…±ä¸‹è½½ {downloaded_count} ä¸ªæ–‡æ¡£ã€‚")


def main():
    print("ğŸ“˜ å¼€å§‹æ‰¹é‡ä¸‹è½½ä½å»ºéƒ¨è§„ç« åº“â€˜æ–‡å­—ç‰ˆâ€™Wordæ–‡æ¡£åŠæå–è¯¦æƒ…é¡µURL...")
    driver.get(base_url)
    time.sleep(5)  # åˆå§‹ç­‰å¾…åŠ è½½

    crawl_pages_with_selenium(max_pages=10)

    print("ğŸ æ‰€æœ‰æ–‡æ¡£å·²ä¿å­˜åœ¨ mohurd_word_documents æ–‡ä»¶å¤¹ä¸­ã€‚")
    print("ğŸ“„ æ‰€æœ‰è¯¦æƒ…é¡µé“¾æ¥å·²å†™å…¥ policy_detail_urls.txt æ–‡ä»¶ä¸­ã€‚")
    driver.quit()


if __name__ == "__main__":
    main()